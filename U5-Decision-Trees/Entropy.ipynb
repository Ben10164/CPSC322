{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['level', 'lang', 'tweets', 'phd', 'interviewed_well']\n",
    "data = [\n",
    "    ['Senior', 'Java', 'no', 'no', 'False'],\n",
    "    ['Senior', 'Java', 'no', 'yes', 'False'],\n",
    "    ['Mid', 'Python', 'no', 'no', 'True'],\n",
    "    ['Junior', 'Python', 'no', 'no', 'True'],\n",
    "    ['Junior', 'R', 'yes', 'no', 'True'],\n",
    "    ['Junior', 'R', 'yes', 'yes', 'False'],\n",
    "    ['Mid', 'R', 'yes', 'yes', 'True'],\n",
    "    ['Senior', 'Python', 'no', 'no', 'False'],\n",
    "    ['Senior', 'R', 'yes', 'no', 'True'],\n",
    "    ['Junior', 'Python', 'yes', 'no', 'True'],\n",
    "    ['Senior', 'Python', 'yes', 'yes', 'True'],\n",
    "    ['Mid', 'Python', 'no', 'yes', 'True'],\n",
    "    ['Mid', 'Java', 'yes', 'no', 'True'],\n",
    "    ['Junior', 'Python', 'no', 'yes', 'False'],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(level, lang, tweets, phd):\n",
    "    if level == \"Senior\":  # 5 cases\n",
    "        # told to split on tweets\n",
    "        if tweets == \"yes\":  # 2 cases\n",
    "            interviewed_well = True  # leaf node (2/5)\n",
    "        elif tweets == \"no\":  # 3 cases\n",
    "            interviewed_well = False  # leaf node (3/5)\n",
    "    elif level == \"Junior\":  # 5 cases\n",
    "        # told to split on phd\n",
    "        if phd == \"yes\":  # 2 cases\n",
    "            interviewed_well = False  # leaf node (2/5)\n",
    "        elif phd == \"no\":  # 3 cases\n",
    "            interviewed_well = True  # leaf node (3/5)\n",
    "    else:  # 4 cases\n",
    "        interviewed_well = True  # leaf node (4/14)\n",
    "    return interviewed_well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "X_1 = [\"Junior\",\"Java\",\"yes\",\"no\"]\n",
    "X_2 = [\"Junior\",\"Java\",\"yes\",\"yes\"]\n",
    "X_1_pred = decision_tree(*X_1) # the star makes the function take the arguments of the list as a tuple  \n",
    "X_2_pred = decision_tree(*X_2) # the star makes the function take the arguments of the list as a tuple\n",
    "print(X_1_pred, X_2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to do better than `random` for attribute selections.  \n",
    "This is where Entropy comes in!  \n",
    "## Entropy:\n",
    "- a measure of uncertainty\n",
    "- goal: minimize uncertainty in order to maximize certainty\n",
    "- result: we can get closer to making a leaf node faster\n",
    "$$E = -\\sum_{i=1}^{n}p_i log_2(p_i)$$\n",
    "* What the formula is saying:\n",
    "    * Since $0 < p_i \\leq 1$, we know that $-p_i log_2(p_i) \\geq 0$ is positive\n",
    "    * e.g., for $log_2(0.5) = y$, we have $2^y = \\frac{1}{2}$, which means $y = -1$\n",
    "    * If $p_i = 1$, then $-p_i log_2(p_i) = 0$\n",
    "    * $E$ has the highest value when labels are equally distributed\n",
    "\n",
    "<img src=\"figures/entropy_graph.png\" width=\"300\"/>\n",
    "\n",
    "Since we want a small E, we want $P_{i}$ to be close to 1 or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the attribute that maximizes information gain\n",
    "* Information Gain = $E_{start} - E_{new}$\n",
    "    * At each partition, pick attribute with highest information gain\n",
    "    * That is, split on attribute with greatest reduction in entropy\n",
    "    * Which means find attribute with smallest $E_{new}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 3\n",
    "What is $E$ for the following distributions? Recall: $E = -\\sum_{i=1}^{n}p_i log_2(p_i)$\n",
    "1. $p_{yes} = 3/8$ and $p_{no} = 5/8$\n",
    "1. $p_{yes} = 2/8$ and $p_{no} = 6/8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9544340029249649\n",
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# 1.1\n",
    "p_yes = 3 / 8\n",
    "p_no = 5 / 8\n",
    "E = -(p_yes * math.log(p_yes, 2) + p_no * math.log(p_no, 2))\n",
    "print(E)\n",
    "\n",
    "# 1. 2\n",
    "p_yes = 2 / 8\n",
    "p_no = 6 / 8\n",
    "E = -(p_yes * math.log(p_yes, 2) + p_no * math.log(p_no, 2))\n",
    "print(E)\n",
    "\n",
    "# notice how 3/8 and 5/8 are both further from 0 or 1 than 2/8 and 6/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since E is smaller for the 2/8 and 6/8 case, we chose this distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9402859586706309\n",
      "0.9402859586706309\n"
     ]
    }
   ],
   "source": [
    "# lab task 4 continues to use the data as lab task 1\n",
    "\n",
    "# 9 interviewed well, 5 did not\n",
    "\n",
    "Estart = -(5/14 * math.log(5/14,2)) - (9/14 * math.log(9/14,2))\n",
    "print(Estart)\n",
    "\n",
    "Estart = -(5/14 * math.log(5/14,2) + 9/14 * math.log(9/14,2))\n",
    "print(Estart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n",
      "0.0\n",
      "0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "# 3/5 of the seiniors have a class label false\n",
    "E_seniors = -(3/5 * math.log(3/5,2)) - (2/5 * math.log(2/5,2))\n",
    "# 4/4 of the mid have a class label of true\n",
    "E_mid = 4/4 * math.log(4/4,2) # UH OH! you cant take the log of 0, so we dont have to do the 0/4 part\n",
    "# 2/5 of the juniors had the class label false\n",
    "E_juniors = -(2/5 * math.log(2/5,2)) - (3/5 * math.log(3/5,2))\n",
    "\n",
    "print(E_seniors)\n",
    "print(E_mid)\n",
    "print(E_juniors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6935361388961918\n"
     ]
    }
   ],
   "source": [
    "# new entropy if we split on the level\n",
    "E_new_level = (5/14) * E_seniors + (4/14) * E_mid + (5/14) * E_juniors\n",
    "print(E_new_level) # we want to find the attribute to minimize this"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f8c5c8ab154ffd7b7cf769370d90abd279d12a3d937a702f83e9fc02204b3d3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: `print_decision_rules()`\n",
    "Finish the `print_decision_rules()` method of `MyDecisionTreeClassifier` that prints out the rules inferred from a decision tree created from a call to `fit()`. Your rules should take the form:\n",
    "\n",
    "```\n",
    "IF att0 == val AND ... THEN class = label\n",
    "IF att1 == val AND ... THEN class = label\n",
    "...\n",
    "```\n",
    "\n",
    "Where \"att0\", \"att1\", ... and \"class\" are replaced with the contextual attribute names and class name if the keyword argument `attribute_names` is not None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: üèÄ Basketball Winner Classification üèÄ (25 pts)\n",
    "Create a decision tree classifier for an NCAA basketball tournament dataset. This is a dataset that I extracted from the [Kaggle March Machine Learning Mania 2022 - Men‚Äôs](https://www.kaggle.com/c/mens-march-mania-2022/data) competition. The original Kaggle dataset has LOTS of data (check it out)!! With some pre-processing of the original dataset, I've created tournament_games2016-2021.csv. This file contains game matchups for 334 tournament games from the last 5 years. Some notes:\n",
    "1. The dataset covers games in the 2016, 2017, 2018, 2019, and 2021 tournaments (there was no tournament in 2020 due to COVID-19)\n",
    "1. There are 67 games in a tournament. 67 games * 5 years = 335 games in total. In 2021, the VCU-Oregon was [ruled a \"no-contest\"](https://www.ncaa.com/news/basketball-men/article/2021-03-20/vcu-oregon-game-ruled-no-contest-due-covid-19-protocols) due to a COVID-19 protocol violation. Therefore, the dataset has 335 - 1 = 334 tournament games\n",
    "1. Each game matchup consists of two teams. Randomly, I assigned one team to be the home (\"H\") team and one team to be the away (\"A\") team to get near equal class distribution.\n",
    "1. Each game matchup includes the game \"Season\", \"HomeTeam\", and \"AwayTeam\" attribute values. I included these to help us humans interpret the game context. These attributes are (likely) too specific to use as features and therefore **you should remove them before classification**.\n",
    "\n",
    "Now for the features... DISCLAIMER: I don't know much about basketball, I don't follow the teams closely, I don't know the important stats, and I have no intuition about this stuff; however, I wanted to make a simple, up-to-date dataset that you could play with (Gonzaga students generally love basketball!!) So, with my limited amount of basketball knowledge (and time), I extracted some VERY simple features from the Kaggle data to describe a game matchup. They are:\n",
    "1. RegularEndingWStreak: which team (\"H\" or \"A\") has the *numerically higher* longest winning streak at the end of this tournament game's corresponding regular season\n",
    "1. RegularSeasonHomePercentageWon: which team (\"H\" or \"A\") has the *numerically higher* home game win percentage during this tournament game's corresponding regular season\n",
    "1. RegularSeasonAwayPercentageWon: which team (\"H\" or \"A\") has the *numerically higher* away game win percentage during this tournament game's corresponding regular season\n",
    "1. RegularSeasonFGPercentMean: which team (\"H\" or \"A\") has the *numerically higher* field goal percentage during this tournament game's corresponding regular season\n",
    "1. RegularSeasonFG3PercentMean: which team (\"H\" or \"A\") has the *numerically higher* 3-pointer percentage during this tournament game's corresponding regular season\n",
    "1. RegularSeasonTOMean: which team (\"H\" or \"A\") has the *numerically higher* turnover percentage during this tournament game's corresponding regular season\n",
    "1. RegularSeasonStlMean: which team (\"H\" or \"A\") has the *numerically higher* accomplished steals percentage during this tournament game's corresponding regular season\n",
    "1. LastOrdinalRank: which team (\"H\" or \"A\") has the *numerically higher* Kenneth Massey ordinal rank at the end of this tournament game's corresponding regular season\n",
    "1. TournamentSeed: which team (\"H\" or \"A\") has the *numerically higher* seed for this tournament\n",
    "1. PlayedBefore: which team (\"H\" or \"A\") won if these two teams played during this tournament game's corresponding regular season (\"N\" if these two teams did not play during this tournament game's corresponding regular season)\n",
    "1. **Winner**: which team (\"H\" or \"A\") won this tournament game\n",
    "    1. This is the **class** we are trying to predict\n",
    "    \n",
    "Since each feature is categorical, you do not need to perform any discretization, etc. The following two steps below have you exploring different subsets. For each step, create a dummy, kNN, Naive Bayes, and decision tree classifier to predict the game winner. Test your classifier using stratified k-fold cross-validation (with k = 10). Format your results as per PA6 and compare your results using:\n",
    "1. Accuracy and error rate\n",
    "1. Precision, recall, and F1 measure\n",
    "1. Confusion matrices\n",
    "\n",
    "### Step 1: Using only the TournamentSeed Feature\n",
    "This provides a baseline set of results. If you always choose the team with the better seed, how often are you right?\n",
    "\n",
    "### Step 2: Using a Feature Subset of your Choosing\n",
    "There are subsets that do *slightly* better than TournamentSeed alone. I challenge you to find them!\n",
    "* Note: because decision trees tend to overfit to training data, I recommend you start with only 2-5 features in your subsets, otherwise your decision rule output will be quite long!\n",
    "* Note: I anticipate the tournament seeds are based on these simple features (plus more, plus expert intuition), so don't expect results much better than using TournamentSeed alone. If you love basketball and data science though, I encourage you to extract additional features you think would be predictive as a fun side project. Perhaps compete in the Kaggle competition next year! You've got a whole year to \"train\" hahhhaa ü§ì\n",
    "\n",
    "Lastly, print out the rules inferred from your decision tree classifiers when trained over the entire dataset (as opposed to the cross validation trees) with your \"best\" feature subset. Based on the rules, determine ways your trees can/should be pruned. Note you do not need to write code to perform pruning, just explain how they can be pruned and give the resulting \"pruned\" rules"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# uncomment once you paste your mypytable.py into mysklearn package\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "# uncomment once you paste your myclassifiers.py into mysklearn package\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: ðŸš¢ Titanic Classification ðŸš¢ (25 pts) \n",
    "The titanic dataset (included in the `input_data` directory) consists of instances representing passengers aboard the Titanic ship that sank in the North Atlantic Ocean on 15 April 1912. The dataset has three attributes describing a passenger (class, age, sex) and a binary class label (survived; 1490 \"yes\" and 711 \"no\") denoting whether the passenger survived the shipwreck or not.\n",
    "\n",
    "Write a Jupyter Notebook (pa6.ipynb) that uses your `mysklearn` package to build Naive Bayes and $k$-nearest neighbor classifiers to predict survival from the titanic dataset **using stratified k-fold cross validation (with k = 10)**. Your classifiers should use class, age, and sex attributes to determine the survival value. Note that since that class, age, and sex are categorical attributes, you will need to update your kNN implementation to properly compute the distance between categorical attributes. See the [B Nearest Neighbors Classification](https://github.com/GonzagaCPSC322/U4-Supervised-Learning/blob/master/B%20Nearest%20Neighbor%20Classification.ipynb) notes on Github for how to go about doing this.\n",
    "\n",
    "How well does $k$NN, Dummy, and Naive Bayes classify the titanic dataset? For each classifier, report the following:\n",
    "1. Accuracy and error rate\n",
    "1. Precision, recall, and F1 measure\n",
    "1. Confusion matrices\n",
    "\n",
    "In the Notebook, describe the steps, log any assumptions and/or issues you had in doing the steps, and provide insights on the results. All re-usable utility functions should be separate from your Notebook in an appropriate module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The headers are: ['class', 'age', 'sex', 'survived']\n"
     ]
    }
   ],
   "source": [
    "# the first thing we are going to do is import the data from the file\n",
    "file_path = \"input_data/titanic.csv\"\n",
    "table = MyPyTable().load_from_file(file_path)\n",
    "headers = table.column_names\n",
    "data = table.data\n",
    "\n",
    "print(\"The headers are:\", headers)\n",
    "# print(\"The data is:\",data) # commented out for output space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need to make what X and y are\n",
    "# y will be survived\n",
    "# X will be everything else [indexes 0, 1, 2]\n",
    "y = table.get_column(\"3\")\n",
    "X = [row[:3] for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the naive bayes classifier is: 0.5\n",
      "The confusion matrix of the naive bayes classifier is: [[711, 0], [711, 0]]\n",
      "The binary F1 report of the naive bayes classifier is: 0\n"
     ]
    }
   ],
   "source": [
    "# first we will make a Naive Bayes classifier\n",
    "naive_bayes_clf = MyNaiveBayesClassifier()\n",
    "\n",
    "# BUT BEFORE WE FIT, we need to amke the training and testing set\n",
    "# we will be usi ng stratified k-fol cross validations (with k = 10)\n",
    "X_train_folds, X_test_folds = myevaluation.stratified_kfold_cross_validation(X,y,10,0)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for fold in X_train_folds:\n",
    "    for index in fold:\n",
    "        X_train.append(data[index][:3])\n",
    "        y_train.append(data[index][3])\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for fold in X_test_folds:\n",
    "    for index in fold:\n",
    "        X_test.append(data[index][:3])\n",
    "        y_test.append(data[index][3])\n",
    "    \n",
    "\n",
    "\n",
    "naive_bayes_clf.fit(X_train, y_train)\n",
    "y_predicted = (naive_bayes_clf.predict(X_test))\n",
    "\n",
    "print(\"The accuracy of the naive bayes classifier is:\", myevaluation.accuracy_score(y_test, y_predicted))\n",
    "print(\"The confusion matrix of the naive bayes classifier is:\", myevaluation.confusion_matrix(y_test, y_predicted, [\"yes\", \"no\"]))\n",
    "print(\"The binary F1 report of the naive bayes classifier is:\", myevaluation.binary_f1_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the kNN classifier is: 0.5\n",
      "The confusion matrix of the kNN classifier is: [[711, 0], [711, 0]]\n",
      "The binary F1 report of the kNN classifier is: 0\n"
     ]
    }
   ],
   "source": [
    "# first we will make a Naive Bayes classifier\n",
    "kNN_clf = MyKNeighborsClassifier()\n",
    "\n",
    "# BUT BEFORE WE FIT, we need to amke the training and testing set\n",
    "# we will be usi ng stratified k-fol cross validations (with k = 10)\n",
    "X_train_folds, X_test_folds = myevaluation.stratified_kfold_cross_validation(X,y,10,0)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for fold in X_train_folds:\n",
    "    for index in fold:\n",
    "        X_train.append(index)\n",
    "        y_train.append(data[index][3])\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for fold in X_test_folds:\n",
    "    for index in fold:\n",
    "        X_test.append(index)\n",
    "        y_test.append(data[index][3])\n",
    "    \n",
    "\n",
    "\n",
    "kNN_clf.fit(X_train, y_train)\n",
    "y_predicted = (naive_bayes_clf.predict(X_test))\n",
    "\n",
    "print(\"The accuracy of the kNN classifier is:\", myevaluation.accuracy_score(y_test, y_predicted))\n",
    "print(\"The confusion matrix of the kNN classifier is:\", myevaluation.confusion_matrix(y_test, y_predicted, [\"yes\", \"no\"]))\n",
    "print(\"The binary F1 report of the kNN classifier is:\", myevaluation.binary_f1_score(y_test, y_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

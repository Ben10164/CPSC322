{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "* ensemble learning: a collection of \"weak\" learners that work together to make predictions\n",
    "* together, the \"weak\" learners for a \"stronger\" learner\n",
    "    * however, this is not guaranteed to be \"stronger\" than any one learner\n",
    "\n",
    "#### Ensemble Classification:\n",
    "* Our weak learners are just weak classifiers that work together to make classifications via voting strategy\n",
    "    * voting strategies: \n",
    "        1. simple majority voting (we will use this in the project)\n",
    "        1. weighted majority voting\n",
    "        1. track record voting (weighted majority voting, while also using historical information from previous attempts)\n",
    "* what are some ways to generate \"weak\" learners:\n",
    "    1. Generate a classifier (tree) using \"different\" training data\n",
    "    1. Generate a classifier (tree) using \"different\" attribute selection techniuques (random, entrppu, gini, etc...)\n",
    "    1. Generate a classifier (tree) using \"different\" attribute subsets \n",
    "    1. Others??? creative\n",
    "* recall: what are the teachniques for train and test sets:\n",
    "1. **holdout method**\n",
    "1. random subsampling \n",
    "1. k-fold cross validation (and its variants)\n",
    "1. **bootstrap method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N,M,F\n",
    "Let `N` be the number of \"weak\" learners in our ensemble:  \n",
    "* example: `N`=100 means that there are 100 weak learners that work together (vote) to make classifications\n",
    "* homogenous ensemble: all the learners are the same type\n",
    "    * this is called a random forest (we will use this in the project)\n",
    "* heterogeneous ensemble: a mix of types\n",
    "\n",
    "Let `M` be the number of \"better\" learners from the `N` \"weak\" learners that we retain to form our final ensemble\n",
    "* `M` < N`\n",
    "* we use the weak learners together making strong learners, and we use the best `M` strong learners in order to form our final ensemble\n",
    "\n",
    "Let `F` be the size of random attribute subsets\n",
    "* `F` >= 2\n",
    "* the higher the `F` is, the more diverse the results will be because it looks at more attributes \n",
    "\n",
    "bagging\n",
    "* bagging: bootstrap aggregating \n",
    "* an ensemble method for creating N weak learners and retainging the M best learners to form an ensemble\n",
    "* basic approach:\n",
    "    1. divide the dataset into a test set and \"remainder\" set \n",
    "        * remainder set is not exactly a training set, but you can still use the train_test_split function we made\n",
    "    1. using the remainder set, sample `N` boostrap samples for one of each $N$ trees (note: this is a different N than the sample size)\n",
    "        * for each sample (used to build 1 tree):\n",
    "            * ~63.2% of instances will be sampled into training set\n",
    "            * ~36.8% of instances will not (form VALIDATION SET)\n",
    "        * BOOSTRAP\n",
    "    1. Measure the performance of each tree on its validation set using some performacne measure, and retain the best `M` trees based on their performance scores\n",
    "        * accuracy Score\n",
    "        * precision score\n",
    "        * recall score\n",
    "        * F1 score\n",
    "    1. Using majority voting, make `M` predictions from the `M` trees for each unseen instance. The majority vote for each is the given instances prediction\n",
    "* what are the advantages of this?\n",
    "    1. simple idea, simple to implement \n",
    "    1. reduces overfitting\n",
    "    1. typically improves accuracy (but not guaranteed) \n",
    "    1. reduces the variance across classifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N=100\n",
    "# size of remainder set = 10, \n",
    "#     in that set, youy do the whole 63~ stuff\n",
    "# this is in a loop that goes on 100 times creating n trees\n",
    "#     all of which having being trained by different samples "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f8c5c8ab154ffd7b7cf769370d90abd279d12a3d937a702f83e9fc02204b3d3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

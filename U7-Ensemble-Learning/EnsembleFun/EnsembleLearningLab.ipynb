{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# pasted from DecisionTreeFun\n",
    "header = [\"level\", \"lang\", \"tweets\", \"phd\"]\n",
    "attribute_domains = {\"level\": [\"Senior\", \"Mid\", \"Junior\"], \n",
    "    \"lang\": [\"R\", \"Python\", \"Java\"],\n",
    "    \"tweets\": [\"yes\", \"no\"], \n",
    "    \"phd\": [\"yes\", \"no\"]}\n",
    "X = [\n",
    "    [\"Senior\", \"Java\", \"no\", \"no\"],\n",
    "    [\"Senior\", \"Java\", \"no\", \"yes\"],\n",
    "    [\"Mid\", \"Python\", \"no\", \"no\"],\n",
    "    [\"Junior\", \"Python\", \"no\", \"no\"],\n",
    "    [\"Junior\", \"R\", \"yes\", \"no\"],\n",
    "    [\"Junior\", \"R\", \"yes\", \"yes\"],\n",
    "    [\"Mid\", \"R\", \"yes\", \"yes\"],\n",
    "    [\"Senior\", \"Python\", \"no\", \"no\"],\n",
    "    [\"Senior\", \"R\", \"yes\", \"no\"],\n",
    "    [\"Junior\", \"Python\", \"yes\", \"no\"],\n",
    "    [\"Senior\", \"Python\", \"yes\", \"yes\"],\n",
    "    [\"Mid\", \"Python\", \"no\", \"yes\"],\n",
    "    [\"Mid\", \"Java\", \"yes\", \"no\"],\n",
    "    [\"Junior\", \"Python\", \"no\", \"yes\"]\n",
    "]\n",
    "\n",
    "y = [\"False\", \"False\", \"True\", \"True\", \"True\", \"False\", \"True\", \"False\", \"True\", \"True\", \"True\", \"True\", \"True\", \"False\"]\n",
    "# stitch X and y together to make one table\n",
    "table = [X[i] + [y[i]] for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 1\n",
    "Write a bootstrap function to return a random sample of rows with\n",
    "replacement:\n",
    "\n",
    "```python\n",
    "def bootstrap(table):\n",
    "    return [table[randint(0,len(table)-1)] for _ in len(table)]\n",
    "```\n",
    "\n",
    "Note: `randint(i, j)` returns $n$ such that $i \\leq n \\leq j$\n",
    "\n",
    "Note that instead of using bootstrapping for testing ...\n",
    "* We are using it here to **create** the ensemble for prediction\n",
    "* i.e., our classifier = set of classifiers over subsamples of original dataset\n",
    "* We are not using bootstrapping for testing in this case\n",
    "\n",
    "Some advantages of bagging (bootstrap aggregation)\n",
    "* Simple idea, simple to implement\n",
    "* Can help deal with overfitting and noisy data (outliers)\n",
    "* Can increase accuracy by reducing variance of individual classifiers\n",
    "\n",
    "### Random Forests\n",
    "Basic Idea\n",
    "* Generate many different decision trees (a \"forest\" of trees) ... $N$ trees\n",
    "\n",
    "Q: What are ways we could do this?\n",
    "* Use bagging (bootstrap aggregation)\n",
    "* Randomly select attributes (many possible trees!)\n",
    "* Use different attribute selection approaches (Entropy, GINI, ...)\n",
    "* Use a subset of attributes for each tree\n",
    "* And so on\n",
    "\n",
    "Random Forests approach:\n",
    "* Build each tree using bagging (so different data sample used for each tree)\n",
    "* At each node, select attribute from a random subset of available attributes... subset size $F$\n",
    "* Use entropy to select attribute to (split) partition on\n",
    "* Select the \"best\" subset of random trees to use in ensemble ... $M \\subset N$\n",
    "\n",
    "Note that $N$, $M$, and $F$ are all parameters of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 2\n",
    "Define a python function that selects F random attributes from an attribute list\n",
    "\n",
    "```python\n",
    "def random_attribute_subset(attributes, F):\n",
    "    # shuffle and pick first F\n",
    "    shuffled = attributes[:] # make a copy\n",
    "    random.shuffle(shuffled)\n",
    "    return shuffled[:F]\n",
    "```\n",
    "* `shuffle()` performs in-place rearrangement (permutation) of given sequence\n",
    "\n",
    "### The Random Forest Procedure\n",
    "1. Divide $D$ into a test and remainder set\n",
    "    * Take 1/3 for test set, 2/3 for remainder set\n",
    "    * Ensure test set has same distribution of class labels as $D$ (\"stratified\")\n",
    "    * Randomly select instances when generating test set\n",
    "2. Create $N$ bootstrap samples from remainder set\n",
    "    * Each results in a **training** (63%) and **validation** (36%) set\n",
    "    * Build and test a classifier for each of the N bootstrap samples\n",
    "    * Each classifier is a decision tree using $F$-sized random attribute subsets\n",
    "    * Determine accuracy of classifier using validation set\n",
    "3. Pick the $M$ best classifiers generated in step 2\n",
    "4. Use test set from step 1 to determine performance of the ensemble of $M$ classifiers (using simple majority voting)\n",
    "\n",
    "Again note: $N$, $M$, and $F$ are parameters (in addition to $D$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lang', 'phd']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def compute_random_subset(values, num_values):\n",
    "    # you can use np.random.choice(), with replace=False\n",
    "    values_copy = values[:] #shallow copy \n",
    "    np.random.shuffle(values_copy) # thios is inplace\n",
    "    return values_copy[:num_values]\n",
    "\n",
    "F = 2\n",
    "print(compute_random_subset(header,F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 3 (For Extra Practice)\n",
    "Assume we have a dataset with 4 attributes ($a_1$, $a_2$, $a_3$, $a_4$) where each attribute has two possible values ($v_1$ and $v_2$) and attribute $a_5$ contains class labels with two possible values ($yes$ and $no$). Using random attribute subsets of size 2:\n",
    "1. Give an example of a complete decision tree that could be generated using the random forest approach\n",
    "1. Show the random attribute subset for each attribute node in the tree."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
